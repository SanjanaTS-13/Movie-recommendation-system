# -*- coding: utf-8 -*-
"""Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UHyvzt3iktlLhI3L1EFXHNmnGYO2lUnS
"""

import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder
import boto3

# Load the CSV file
movies_file = 's3://movierecommendations3/movies.csv'  # Replace this with the path to your actual file
movies_df = pd.read_csv(movies_file)

# Clean the 'genre' column
def clean_genres(genre):
    try:
        return eval(genre) if isinstance(genre, str) else []
    except:
        return []

movies_df['genre'] = movies_df['genre'].apply(clean_genres)

# Remove rows with empty genres
movies_df = movies_df[movies_df['genre'].apply(len) > 0]

# One-hot encode genres using MultiLabelBinarizer
mlb = MultiLabelBinarizer()
genres_encoded = mlb.fit_transform(movies_df['genre'])

# Create a DataFrame of the encoded genres
genres_df = pd.DataFrame(genres_encoded, columns=mlb.classes_)

# Concatenate the encoded genres back to the movies dataframe
movies_df = pd.concat([movies_df, genres_df], axis=1)

# Initialize LabelEncoder for movie titles
le = LabelEncoder()
movies_df['movie_label'] = le.fit_transform(movies_df['original_title'])

label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
# print(label_mapping)

# Verify columns before selecting
print("Available columns in the DataFrame:")
print(movies_df.columns)

# Keep only the one-hot encoded genres and the encoded movie labels
try:
    processed_data = movies_df[list(mlb.classes_) + ['movie_label']]
except KeyError as e:
    print(f"KeyError: {e}")
    print(f"Available columns: {movies_df.columns.tolist()}")

# Save the processed data locally
local_processed_data_path = "preprocessed_movies.csv"
processed_data.to_csv(local_processed_data_path, index=False)

# Upload the processed data to S3
s3 = boto3.client('s3')
s3_bucket = 'movierecommendations3'  # Replace with your S3 bucket name
s3_key = 'preprocessed_movies.csv'  # S3 file key
s3.upload_file(local_processed_data_path, s3_bucket, s3_key)

# S3 URI of the uploaded file
s3_uri = f"s3://{s3_bucket}/{s3_key}"
print(f"Processed data uploaded to: {s3_uri}")

import json
import boto3

# Save label_mapping as JSON
label_mapping = {str(k): int(v) for k, v in label_mapping.items()}
label_mapping_path = 'label_mapping.json'
with open(label_mapping_path, 'w') as f:
    json.dump(label_mapping, f)

# Upload to S3
s3 = boto3.client('s3')
s3.upload_file(label_mapping_path, 'movierecommendations3', 'label_mapping.json')
print("label_mapping saved and uploaded to S3.")

import sagemaker
from sagemaker import get_execution_role
from sagemaker.inputs import TrainingInput
from sagemaker.estimator import Estimator

train_data = f's3://movierecommendations3/preprocessed_movies.csv'

sagemaker_session = sagemaker.Session()

# Define the XGBoost estimator for training
xgb = Estimator(
    image_uri=sagemaker.image_uris.retrieve('xgboost', sagemaker_session.boto_region_name, version='1.0-1'),
    role = 'arn:aws:iam::703671913539:role/SageMakerMovie',
    instance_count=1,
    instance_type='ml.m5.xlarge',  # Choose instance type for training
    output_path = 's3://movierecommendations3/output/model',  # Path to store model
    sagemaker_session=sagemaker_session
)

# Set hyperparameters for XGBoost (regression or classification)
xgb.set_hyperparameters(
    objective="multi:softmax",
    num_class=len(label_mapping),
    num_round=5,
    eta=0.3,
    max_depth=3,
    tree_method="hist"
)

print("Number of Classes (len(label_mapping)):", len(label_mapping))
print("Unique Labels in Training Data:", movies_df['movie_label'].nunique())

#### Define input data for training
train_input = TrainingInput(train_data, content_type='csv')

# Train the model
xgb.fit({'train': train_input})

predictor = xgb.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.xlarge',
    endpoint_name="movie-recommendation-AI",
)

endpoint_name = predictor.endpoint_name
print(f"Model deployed at endpoint: {endpoint_name}")



